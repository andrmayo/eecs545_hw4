{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zNmLmqrJAXXp"
   },
   "source": [
    "# EECS 545 (WN 2025) Homework 4: RNNs and Image Captioning\n",
    "\n",
    "<span class=\"instruction\">Before starting the assignment, please fill in the following cell.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name and email: Andrew Mayo <acmayo@umich.edu>\n"
     ]
    }
   ],
   "source": [
    "###################################################################\n",
    "# Enter your first and last name, e.g. \"John Doe\"                 #\n",
    "# for example                                                     #\n",
    "__NAME__ = \"Andrew Mayo\"                                         #\n",
    "__UNIQID__ = \"acmayo\"                                       #\n",
    "###################################################################\n",
    "###################################################################\n",
    "#                        END OF YOUR CODE                         #\n",
    "###################################################################\n",
    "\n",
    "print(f\"Your name and email: {__NAME__} <{__UNIQID__}@umich.edu>\")\n",
    "assert __NAME__ and __UNIQID__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hbe3wUpVAjma"
   },
   "source": [
    "# RNNs and Image Captioning\n",
    "In this notebook, you will test your RNN implementation from `rnn.py` on the coco image captioning dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eYE9thuXn4zP"
   },
   "source": [
    "## Setup code\n",
    "Before getting started, we need to run some boilerplate code to set up our environment. You'll need to rerun this setup code each time you start the notebook. Let's start by checking whether we are using Python 3.11 or higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QqEfH2Rpn9J3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are good to go\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if sys.version_info[0] < 3:\n",
    "    raise Exception(\"You must use Python 3\")\n",
    "\n",
    "if sys.version_info[1] < 11:\n",
    "    print(\"Autograder will execute your code based on Python 3.11 environment. Please use Python 3.11 or higher to prevent any issues\")\n",
    "    print(\"You can create a conda environment with Python 3.11 like 'conda create --name eecs545 python=3.11'\")\n",
    "    raise Exception(\"Python 3 version is too low: {}\".format(sys.version))\n",
    "else:\n",
    "    print(\"You are good to go\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, run this cell load the [autoreload](https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html) extension. This allows us to edit `.py` source files, and re-import them into the notebook for a seamless editing and debugging experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GaCqHOm9oPB3"
   },
   "source": [
    "Then, we run some setup code for this notebook: Import some useful packages and increase the default figure size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "oCaNVx6JoWid",
    "outputId": "2133e4c6-8a6e-4ea3-dd97-23ad471ba2b0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install required libraries\n",
    "# !pip install numpy==1.24.1 matplotlib==3.6.2 scikit-learn==1.2.0 h5py==3.8.0 imageio==2.25.1\n",
    "\n",
    "# import libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set figure size\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O3EvIZ0uAOVN",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style type=\"text/css\">\n",
       "  .instruction { background-color: yellow; font-weight:bold; padding: 3px; }\n",
       "</style>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display_html, HTML\n",
    "\n",
    "display_html(HTML('''\n",
    "<style type=\"text/css\">\n",
    "  .instruction { background-color: yellow; font-weight:bold; padding: 3px; }\n",
    "</style>\n",
    "'''));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the following cell allow us to import from `rnn_layers.py` and `rnn.py`. If it works correctly, it should print the message:\n",
    "```Hello from rnn_layers.py``` and ```Hello from rnn.py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from rnn_layers.py!\n",
      "Hello from rnn.py!\n"
     ]
    }
   ],
   "source": [
    "from rnn_layers import hello\n",
    "from rnn import hello as hello2\n",
    "hello()\n",
    "hello2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is COCO?\n",
    "\n",
    "[COCO](https://cocodataset.org/) (Common Objects in COntext) is a large-scale object detection, segmentation, and captioning dataset.\n",
    "\n",
    "COCO has 330K images (>200K labeled)! Labelled images have object segmentations, and captions.\n",
    "\n",
    "![COCO examples](https://cocodataset.org/images/coco-examples.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading COCO\n",
    "\n",
    "Please download the dataset (987Mb) from [this link](https://drive.google.com/file/d/1RjZTIVp4ES1Ewv1QAJgCS_7DYyJa-Yf1/view?usp=sharing) and unzip the folder into the data directory.\n",
    "```\n",
    "HW4 /\n",
    "--| image_captioning.ipynb\n",
    "--| data /\n",
    "----| coco_captioning /\n",
    "------| coco2014_captions.h5\n",
    "------| ...\n",
    "```\n",
    "\n",
    "The dataset contains the preprocessed features and captions from the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: h5py in /home/andrew/.local/lib/python3.12/site-packages (3.13.0)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /home/andrew/.local/lib/python3.12/site-packages (from h5py) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py\n",
    "import coco_utils\n",
    "\n",
    "MAX_TRAIN = 200# We'll train with a very small dataset\n",
    "\n",
    "# Load COCO data from disk; this returns a dictionary\n",
    "small_data = coco_utils.load_coco_data(max_train=MAX_TRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing COCO\n",
    "\n",
    "Let's check out some images in our COCO dataset. You can browse the full dataset [here](https://cocodataset.org/#explore)! My favourite image is [this one](https://cocodataset.org/#explore?id=314526). It turns out when you have a 330K image dataset, some of the images might be really [weird](https://blog.roboflow.com/coco-dataset-image-search/). Check it out!\n",
    "\n",
    "Let's visualize some images from the downloaded COCO. Note we have to load them from the URL since they aren't stored in our dataset. (We save the preprocessed features). Some of these images may not load properly because the URLs are no longer active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def md_table(urls, captions, cols=5):\n",
    "    table = '|   ' * cols + '|\\n' + '|---' * cols + '|\\n'\n",
    "    row = lambda idxs: '| ' + ' | '.join(f'![train image {i}]({urls[i]})' for i in idxs) + ' |'\n",
    "    row_cap = lambda idxs: '| ' + ' | '.join(f'**{str(captions[i])}**' for i in idxs) + ' |'\n",
    "    table += '\\n'.join(row(range(i, i+cols)) + '\\n' + row_cap(range(i, i+cols)) for i in range(0, len(urls), cols))\n",
    "    table = table.replace('<', '\\<')# show angle brackets in MD\n",
    "    table = table.replace('\\<br>', '<br>')# keep <br> as newline\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|   |   |   |   |   |\n",
       "|---|---|---|---|---|\n",
       "| ![train image 0](http://farm4.staticflickr.com/3828/9515372128_e874a589fa_z.jpg) | ![train image 1](http://farm9.staticflickr.com/8093/8362722159_5c76891fba_z.jpg) | ![train image 2](http://farm6.staticflickr.com/5480/9706351564_6b7001868e_z.jpg) | ![train image 3](http://farm5.staticflickr.com/4052/5162428391_872f641b4a_z.jpg) | ![train image 4](http://farm1.staticflickr.com/51/105582257_149d0b1007_z.jpg) |\n",
       "| **\\<START> two people walking down a street holding an umbrella \\<END>** | **\\<START> the plate is filled with meat and vegetables \\<END>** | **\\<START> tennis player in a tennis court \\<UNK> with her tennis racket \\<END>** | **\\<START> a man in a blue shirt holding a white plate with some food on it \\<END>** | **\\<START> a \\<UNK> vase being displayed in a \\<UNK> \\<END>** |\n",
       "| ![train image 5](http://farm9.staticflickr.com/8016/7268221008_3e953f93dd_z.jpg) | ![train image 6](http://farm5.staticflickr.com/4081/4751171578_2137e00d34_z.jpg) | ![train image 7](http://farm9.staticflickr.com/8462/8036487923_12f8c0b71e_z.jpg) | ![train image 8](http://farm8.staticflickr.com/7366/9889727824_ae51c4077f_z.jpg) | ![train image 9](http://farm8.staticflickr.com/7409/10194986376_61bbeac653_z.jpg) |\n",
       "| **\\<START> a person is standing on the water on a \\<UNK> board \\<END>** | **\\<START> a traffic signal with a very big pretty building by it \\<END>** | **\\<START> a very big room with a big pretty clock \\<END>** | **\\<START> many beautiful fruit \\<UNK> line the shelves in the market \\<END>** | **\\<START> a double decker green bus driving down a \\<UNK> road near a lake \\<END>** |\n",
       "| ![train image 10](http://farm4.staticflickr.com/3202/2345517887_36ef2e7551_z.jpg) | ![train image 11](http://farm9.staticflickr.com/8010/7643240678_82fdaa2f60_z.jpg) | ![train image 12](http://farm4.staticflickr.com/3346/3616813166_ff18a41f6f_z.jpg) | ![train image 13](http://farm9.staticflickr.com/8452/8041161824_c6da3831c1_z.jpg) | ![train image 14](http://farm7.staticflickr.com/6121/5917556272_2448eabaa4_z.jpg) |\n",
       "| **\\<START> a large green hotel sign on the corner \\<END>** | **\\<START> \\<UNK> \\<UNK> on a street with traffic behind it \\<END>** | **\\<START> the \\<UNK> stands next to the \\<UNK> as a batter is swinging \\<END>** | **\\<START> a cat is standing on a seat of a toilet \\<END>** | **\\<START> a pizza with \\<UNK> \\<UNK> \\<UNK> and cheese \\<END>** |\n",
       "| ![train image 15](http://farm4.staticflickr.com/3645/3571849552_9afdac5dec_z.jpg) | ![train image 16](http://farm4.staticflickr.com/3117/2636949743_56a878209d_z.jpg) | ![train image 17](http://farm2.staticflickr.com/1414/1366775115_58ff6b6cd3_z.jpg) | ![train image 18](http://farm4.staticflickr.com/3277/3089235227_1560b21f11_z.jpg) | ![train image 19](http://farm9.staticflickr.com/8453/8018484689_ec5a2594d2_z.jpg) |\n",
       "| **\\<START> \\<UNK> a flat toilet in a small bathroom \\<END>** | **\\<START> a \\<UNK> tennis match \\<UNK> \\<UNK> on a grass court \\<END>** | **\\<START> a \\<UNK> with several \\<UNK> pictures on it \\<END>** | **\\<START> a small bathroom with a \\<UNK> looking toilet and sink \\<END>** | **\\<START> a cow with an ear \\<UNK> is standing in tall grass \\<END>** |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown as md\n",
    "n_imgs = 20\n",
    "rng = np.random.default_rng(545)\n",
    "captions, features, img_urls = coco_utils.sample_coco_minibatch(\n",
    "    small_data, split='train', batch_size=n_imgs, seed=545)\n",
    "cap_str = [coco_utils.decode_captions(c, small_data['idx_to_word']) for c in captions]\n",
    "md(md_table(img_urls, cap_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Footnote: Special tokens. \\<START> and \\<END> tokens denote the start and end of the captions.\n",
    "The \\<UNK> token is created during preprocessing and replaces words that do not occur in the dataset very often. Doing this sometimes helps the model learn better since it shrinks the vocabulary!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Next, we'll train an RNN to predict captions on COCO. <span class=\"instruction\">Make sure you have implemented `temporal_fc_forward` and `temporal_fc_backward` in `rnn_layers.py`, and `CaptioningRNN` from `rnn.py`.</span>\n",
    "Read through `captioning_solver.py` to make sure you understand the API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness Checks\n",
    "\n",
    "Like `cnn.ipynb`, we have included some tests to check your implementation. However, we have left the numerical gradient checks blank.\n",
    "These checks are optional, but they may be helpful for debugging. Hint: follow the same steps as done in `cnn.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instructor dx is [[[-3.95940839 -1.50144211 -0.68489275  1.33278083]\n",
      "  [-2.62580199  1.2605177   2.18724271 -5.26377862]\n",
      "  [ 4.11329435  0.60980944 -2.31310215  1.99082021]]\n",
      "\n",
      " [[-2.81130313 -1.49152503 -5.64614596  0.72709276]\n",
      "  [-4.24984996  0.16068659 -1.0610444   0.03341529]\n",
      "  [-0.72765064  0.24485565 -1.0991891  -1.75997374]]]\n",
      "my dx is [[[-3.95940839 -1.50144211 -0.68489275  1.33278083]\n",
      "  [-2.62580199  1.2605177   2.18724271 -5.26377862]\n",
      "  [ 4.11329435  0.60980944 -2.31310215  1.99082021]]\n",
      "\n",
      " [[-2.81130313 -1.49152503 -5.64614596  0.72709276]\n",
      "  [-4.24984996  0.16068659 -1.0610444   0.03341529]\n",
      "  [-0.72765064  0.24485565 -1.0991891  -1.75997374]]]\n",
      "instructor dw is [[-4.67359018  1.97809184 -1.65588886  2.65158379 -0.09787531]\n",
      " [-4.39925537 -0.4122042  -0.17105269  2.87291261 -2.34867127]\n",
      " [ 1.18387255 -0.63429565 -2.11167034  0.01993428  3.35761225]\n",
      " [-2.2462575   1.83430881  2.51261772  0.31383247 -1.50652707]]\n",
      "my dw is [[-4.67359018  1.97809184 -1.65588886  2.65158379 -0.09787531]\n",
      " [-4.39925537 -0.4122042  -0.17105269  2.87291261 -2.34867127]\n",
      " [ 1.18387255 -0.63429565 -2.11167034  0.01993428  3.35761225]\n",
      " [-2.2462575   1.83430881  2.51261772  0.31383247 -1.50652707]]\n",
      "instructor db is [-0.74259108  1.07852908  3.763827   -2.31054969 -0.98407367]\n",
      "my db is [-0.74259108  1.07852908  3.763827   -2.31054969 -0.98407367]\n"
     ]
    }
   ],
   "source": [
    "from gradient_check import rel_error, eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from rnn_layers import temporal_fc_backward, temporal_fc_forward\n",
    "\n",
    "# OPTIONAL TODO Test with eval_numerical_gradient_array.\n",
    "# Hint: Start by sampling some random test data. (Refer to the rnn_layers.py comments to check the shapes)\n",
    "# Then, call eval_numerical_gradient_array on the test data. (Refer to cnn.ipynb for examples)\n",
    "# Compare the results of eval_numerical_gradient_array with your own backward implementation.\n",
    "\n",
    "rng = np.random.default_rng(1234)\n",
    "\n",
    "n, t, d, m = (2, 3, 4, 5)\n",
    "dout = rng.standard_normal((n, t, m))\n",
    "x = rng.standard_normal((n, t, d))\n",
    "w = rng.standard_normal((d, m))\n",
    "b = rng.standard_normal((m))\n",
    "out = rng.standard_normal((n, t, m))\n",
    "cache = (x, w, b, out)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: temporal_fc_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: temporal_fc_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: temporal_fc_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "dx_mine, dw_mine, db_mine = temporal_fc_backward(dout, cache)\n",
    "\n",
    "print(f\"instructor dx is {dx_num}\")\n",
    "print(f\"my dx is {dx_mine}\")\n",
    "\n",
    "print(f\"instructor dw is {dw_num}\")\n",
    "print(f\"my dw is {dw_mine}\")\n",
    "\n",
    "print(f\"instructor db is {db_num}\")\n",
    "print(f\"my db is {db_mine}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing training loss...\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "TODO: Add your implementation here.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m y = rng.integers(\u001b[32m0\u001b[39m, \u001b[32m10\u001b[39m, size=(\u001b[32m2\u001b[39m, \u001b[32m4\u001b[39m))\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mTesting training loss...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m loss, grads = \u001b[43mtest_rnn_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m np.abs(loss - \u001b[32m7.783337014604364\u001b[39m) < \u001b[32m1e-4\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mloss should be close.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# OPTIONAL TODO Test with eval_numerical_gradient_array.\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Hint: write a loop to check each grad (Refer to cnn.ipynb for examples)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/EECS545/assignments/hw4/rnn.py:156\u001b[39m, in \u001b[36mCaptioningRNN.loss\u001b[39m\u001b[34m(self, features, captions)\u001b[39m\n\u001b[32m    131\u001b[39m loss, grads = \u001b[32m0.0\u001b[39m, {}\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m############################################################################\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[38;5;66;03m# TODO: Implement the forward and backward passes for the CaptioningRNN.   #\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[38;5;66;03m# In the forward pass you will need to do the following.                   #\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    154\u001b[39m \u001b[38;5;66;03m# gradients for self.params[k].                                            #\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m############################################################################\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTODO: Add your implementation here.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m############################################################################\u001b[39;00m\n\u001b[32m    158\u001b[39m \u001b[38;5;66;03m#                             END OF YOUR CODE                             #\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m############################################################################\u001b[39;00m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss, grads\n",
      "\u001b[31mNotImplementedError\u001b[39m: TODO: Add your implementation here."
     ]
    }
   ],
   "source": [
    "from gradient_check import rel_error, eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from rnn import CaptioningRNN\n",
    "\n",
    "# Generate dummy test data\n",
    "wtoi = {f'{i}': i for i in range(7)}# dummy word to index\n",
    "b\n",
    "wtoi['<NULL>'] = 8\n",
    "wtoi['<START>'] = 9\n",
    "wtoi['<END>'] = 10\n",
    "np.random.seed(0)\n",
    "test_rnn_model = CaptioningRNN(\n",
    "      cell_type='rnn',\n",
    "      word_to_idx=wtoi,\n",
    "      input_dim=5,\n",
    "      hidden_dim=4,\n",
    "      wordvec_dim=5,\n",
    ")\n",
    "rng = np.random.default_rng(545)\n",
    "X = rng.standard_normal((2, 5))\n",
    "y = rng.integers(0, 10, size=(2, 4))\n",
    "\n",
    "print('Testing training loss...')\n",
    "loss, grads = test_rnn_model.loss(X, y)\n",
    "\n",
    "assert np.abs(loss - 7.783337014604364) < 1e-4, 'loss should be close.'\n",
    "\n",
    "# OPTIONAL TODO Test with eval_numerical_gradient_array.\n",
    "# Hint: write a loop to check each grad (Refer to cnn.ipynb for examples)\n",
    "\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate dummy test data\n",
    "wtoi = {f'{i}': i for i in range(7)}# dummy word to index\n",
    "wtoi['<NULL>'] = 8\n",
    "wtoi['<START>'] = 9\n",
    "wtoi['<END>'] = 10\n",
    "np.random.seed(1)\n",
    "test_rnn_model = CaptioningRNN(\n",
    "      cell_type='rnn',\n",
    "      word_to_idx=wtoi,\n",
    "      input_dim=5,\n",
    "      hidden_dim=4,\n",
    "      wordvec_dim=5,\n",
    ")\n",
    "rng = np.random.default_rng(545)\n",
    "X = rng.standard_normal((2, 5))\n",
    "\n",
    "print('Testing rnn sample...')\n",
    "captions = test_rnn_model.sample(X, max_length=17)\n",
    "gt_captions = np.array([[1, 0, 7, 2, 1, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
    "       [6, 5, 1, 0, 7, 6, 5, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7]])\n",
    "assert np.all(captions == gt_captions)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn import CaptioningRNN\n",
    "from captioning_solver import CaptioningSolver\n",
    "\n",
    "np.random.seed(0)\n",
    "# Experiment with vanilla RNN\n",
    "small_rnn_model = CaptioningRNN(\n",
    "      cell_type='rnn',\n",
    "      word_to_idx=small_data['word_to_idx'],\n",
    "      input_dim=small_data['train_features'].shape[1],\n",
    "      hidden_dim=512,\n",
    "      wordvec_dim=256,\n",
    ")\n",
    "\n",
    "small_rnn_solver = CaptioningSolver(small_rnn_model, small_data,\n",
    "       update_rule='adam',\n",
    "       num_epochs=58,\n",
    "       batch_size=25,\n",
    "       optim_config={\n",
    "         'learning_rate': 4e-3,\n",
    "       },\n",
    "       lr_decay=0.95,\n",
    "       verbose=True, print_every=10,\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train!\n",
    "\n",
    "Remember to include the plot of your learning curves in the final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_rnn_solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training losses\n",
    "plt.plot(small_rnn_solver.loss_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training loss history')\n",
    "plt.savefig('image_captioning_loss.png', dpi=256)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and Visualization\n",
    "\n",
    "Finally, let's test our model on some validation data. Include your generated train and validation captions in the final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_utils import image_from_url\n",
    "\n",
    "tables = {}\n",
    "for split in ['train', 'val']:\n",
    "    # some images might be deprecated. You may rerun the code several times\n",
    "    # to successfully get the sample images from url.\n",
    "    minibatch = coco_utils.sample_coco_minibatch(\n",
    "        small_data, split=split, batch_size=10, seed=545)\n",
    "    gt_captions, features, urls = minibatch\n",
    "    gt_captions = coco_utils.decode_captions(gt_captions,\n",
    "                                             small_data['idx_to_word'])\n",
    "\n",
    "    sample_captions = small_rnn_model.sample(features)\n",
    "    sample_captions = coco_utils.decode_captions(sample_captions,\n",
    "                                                 small_data['idx_to_word'])\n",
    "\n",
    "    figure_caption = [f'Sample: {sample_cap} <br> GT: {gt_cap}' for sample_cap, gt_cap in zip(sample_captions, gt_captions)]\n",
    "    tables[split] = md_table(urls, figure_caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md('## Training samples \\n ' + tables['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "md('## Validation samples \\n ' + tables['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "eYE9thuXn4zP",
    "CdowvtJen-IP",
    "KtMy3qeipNK3",
    "Hbe3wUpVAjma",
    "lJqim3P1qZgv",
    "ZLdCF3B-AOVT",
    "7XNJ3ydEAOVW",
    "vExP-7n3AOVa",
    "LjAUalCBAOVd",
    "8cPIajWNAOVg",
    "_CsYAv3uAOVi",
    "ixxgq5RKAOVl",
    "OlVbXxmPNzPY",
    "rDNZ8ZAnN7hj",
    "QpSrK3olUfOZ",
    "3zFWkxebWXtu",
    "mVCEro4FAOVq",
    "UG56gKWsAOVv",
    "37R_J2uMP3d-"
   ],
   "name": "two_layer_net.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
